{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kaiyoo88/tutorial-chatbot-rag-langchain?scriptVersionId=205502365\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:01.366291Z","iopub.execute_input":"2024-11-06T03:32:01.367877Z","iopub.status.idle":"2024-11-06T03:32:21.510069Z","shell.execute_reply.started":"2024-11-06T03:32:01.367808Z","shell.execute_reply":"2024-11-06T03:32:21.506818Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 1. Use langchain RAG","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:21.525654Z","iopub.execute_input":"2024-11-06T03:32:21.528856Z","iopub.status.idle":"2024-11-06T03:32:24.813269Z","shell.execute_reply.started":"2024-11-06T03:32:21.52874Z","shell.execute_reply":"2024-11-06T03:32:24.81127Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-API-KEY\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:24.820152Z","iopub.execute_input":"2024-11-06T03:32:24.822723Z","iopub.status.idle":"2024-11-06T03:32:24.839674Z","shell.execute_reply.started":"2024-11-06T03:32:24.822588Z","shell.execute_reply":"2024-11-06T03:32:24.837251Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:24.841782Z","iopub.execute_input":"2024-11-06T03:32:24.842623Z","iopub.status.idle":"2024-11-06T03:32:24.855683Z","shell.execute_reply.started":"2024-11-06T03:32:24.84255Z","shell.execute_reply":"2024-11-06T03:32:24.85326Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFaceHub\n\n# set Korean embedding and llm odel\nhf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n\nhf_llm = HuggingFaceHub(\n    repo_id=\"skt/kogpt2-base-v2\",\n    model_kwargs={\"task\": \"text-generation\"} ## question-answering tasK X. text-generation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:24.861353Z","iopub.execute_input":"2024-11-06T03:32:24.862099Z","iopub.status.idle":"2024-11-06T03:32:48.381323Z","shell.execute_reply.started":"2024-11-06T03:32:24.862032Z","shell.execute_reply":"2024-11-06T03:32:48.379777Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2856/3316765938.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  hf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/tmp/ipykernel_2856/3316765938.py:7: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n  hf_llm = HuggingFaceHub(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import requests\nfrom langchain.schema import Document\nfrom bs4 import BeautifulSoup\n\n# for Wikipedia documents (EN, KO)\n\n# from langchain_community.document_loaders import WikipediaLoader\n\n# By default, English documents (https://en.wikipedia.org))\n# def load_Wiki_docs(query):\n#     loader = WikipediaLoader(query=query, load_max_docs=1) # need !pip install wikipedia\n#     documents = loader.load()\n    \n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=1000,\n#         chunk_overlap=200\n#     )\n#     splits = text_splitter.split_documents(documents)\n    \n#     return splits\n\n\n# For Korean query, get results from Korean wikipedia website and crawl and parse results\ndef load_Korean_wiki_docs(topic):\n    url = f\"https://ko.wikipedia.org/wiki/{topic}\"\n    \n    response = requests.get(url)\n    response.raise_for_status()  # raise Exception when error occurs\n\n    # HTML parsing and extract body contents\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.find('div', {'class': 'mw-parser-output'})  # find div including body contents \n    \n    # Extract contents\n    paragraphs = content.find_all('p')\n    text = \"\\n\".join([p.get_text() for p in paragraphs])  # concat all context in <p> tags \n \n    # convert to Document object (required for LangChain)\n    documents = [Document(page_content=text, metadata={\"source\": url})]\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    splits = text_splitter.split_documents(documents)\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:48.383367Z","iopub.execute_input":"2024-11-06T03:32:48.384544Z","iopub.status.idle":"2024-11-06T03:32:48.554287Z","shell.execute_reply.started":"2024-11-06T03:32:48.384475Z","shell.execute_reply":"2024-11-06T03:32:48.552893Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_vectorstore(splits): \n    vectorstore = Chroma.from_documents(documents=splits, embedding=hf_embeddings)\n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:48.556168Z","iopub.execute_input":"2024-11-06T03:32:48.557164Z","iopub.status.idle":"2024-11-06T03:32:48.563965Z","shell.execute_reply.started":"2024-11-06T03:32:48.557112Z","shell.execute_reply":"2024-11-06T03:32:48.562434Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"topic = \"흑백요리사\"\n# Load wikipedia documents for this topic\nsplits = load_Korean_wiki_docs(topic) \n# Create vectorstore with this fetched docs\nvectorstore = create_vectorstore(splits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:48.565987Z","iopub.execute_input":"2024-11-06T03:32:48.566501Z","iopub.status.idle":"2024-11-06T03:32:50.353028Z","shell.execute_reply.started":"2024-11-06T03:32:48.566456Z","shell.execute_reply":"2024-11-06T03:32:50.351465Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_rag_chain(vectorstore):\n    prompt_template = \"\"\"문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: {context}\n    질문: {question}\n    답변:\"\"\"\n    \n    PROMPT = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    chain_type_kwargs = {\"prompt\": PROMPT}\n\n    # Make context shorter\n    # def short_context(context, max_length=300):\n    #     return context[:max_length] if len(context) > max_length else context\n    \n    # class ShortContextRetriever(BaseRetriever):\n    #     def __init__(self, retriever):\n    #         super().__init__()\n    #         self._retriever = retriever\n        \n    #     def get_relevant_documents(self, query):\n    #         docs = self._retriever.get_relevant_documents(query)\n    #         for doc in docs:\n    #             doc.page_content = short_context(doc.page_content)\n    #         return docs\n    \n    # retriever = ShortContextRetriever(vectorstore.as_retriever())\n\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=hf_llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever(),\n        chain_type_kwargs=chain_type_kwargs,\n        return_source_documents=True\n    )\n    \n    return qa_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:50.355019Z","iopub.execute_input":"2024-11-06T03:32:50.355686Z","iopub.status.idle":"2024-11-06T03:32:50.365592Z","shell.execute_reply.started":"2024-11-06T03:32:50.355638Z","shell.execute_reply":"2024-11-06T03:32:50.36378Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# create langchang RAG chain\nqa_chain = create_rag_chain(vectorstore)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:50.367808Z","iopub.execute_input":"2024-11-06T03:32:50.369093Z","iopub.status.idle":"2024-11-06T03:32:50.388798Z","shell.execute_reply.started":"2024-11-06T03:32:50.369027Z","shell.execute_reply":"2024-11-06T03:32:50.386923Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"question = \"심사위원을 누가 맡았어?\"\n\n# result = qa_chain({\"query\": question})\nresult = qa_chain.invoke({\"query\": question})\n\nprint (\"결과:\")\nprint(result[\"result\"])\n\nprint(\"출처:\")\nfor doc in result[\"source_documents\"]:\n    print(doc.page_content)\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:50.390676Z","iopub.execute_input":"2024-11-06T03:32:50.391172Z","iopub.status.idle":"2024-11-06T03:33:15.32094Z","shell.execute_reply.started":"2024-11-06T03:32:50.391121Z","shell.execute_reply":"2024-11-06T03:33:15.319417Z"}},"outputs":[{"name":"stdout","text":"결과:\n문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n    질문: 심사위원을 누가 맡았어?\n    답변: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원:\n출처:\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n---\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"docs = vectorstore.as_retriever().get_relevant_documents(question)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.322779Z","iopub.execute_input":"2024-11-06T03:33:15.323234Z","iopub.status.idle":"2024-11-06T03:33:15.485525Z","shell.execute_reply.started":"2024-11-06T03:33:15.323191Z","shell.execute_reply":"2024-11-06T03:33:15.484176Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2856/2742417880.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  docs = vectorstore.as_retriever().get_relevant_documents(question)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]')]"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"docs = vectorstore.similarity_search(question, k=4)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.487481Z","iopub.execute_input":"2024-11-06T03:33:15.488034Z","iopub.status.idle":"2024-11-06T03:33:15.574314Z","shell.execute_reply.started":"2024-11-06T03:33:15.487979Z","shell.execute_reply":"2024-11-06T03:33:15.572843Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]')]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# It seems vectorDB loading from embedding model works fine, but seems llm model does not.\n# Some Korean llm model seems to work fine in text-generation task, but for Question-Ansering task, we might need another approach.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.580119Z","iopub.execute_input":"2024-11-06T03:33:15.580617Z","iopub.status.idle":"2024-11-06T03:33:15.586739Z","shell.execute_reply.started":"2024-11-06T03:33:15.580573Z","shell.execute_reply":"2024-11-06T03:33:15.585086Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Use QA pipeline with vectorstor similarity search","metadata":{}},{"cell_type":"code","source":"# import torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model and tokenizer\nmodel_name = \"yjgwak/klue-bert-base-finetuned-squard-kor-v1\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set Q_A pipeline\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.588684Z","iopub.execute_input":"2024-11-06T03:33:15.589173Z","iopub.status.idle":"2024-11-06T03:33:31.421328Z","shell.execute_reply.started":"2024-11-06T03:33:15.589128Z","shell.execute_reply":"2024-11-06T03:33:31.419902Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3833469c3d4426da9a1d156d9619cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb7297378344920a0463e0bf05c9c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e1c3f24f674a569dd6056c36e60f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/246k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866f83b22a1043faa7da0e7efd1222e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d058c19b4f94f08952b0e36694982f6"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Example: define question and context \nquestion = \"오늘 날씨 어때?\"\ncontext = \"오늘의 날씨는 맑고 따뜻한 기온이 유지될 것으로 보입니다.\"\n\n# model chain\nresult = qa_pipeline(question=question, context=context)\n\n# Result\nprint(\"질문:\", question)\nprint(\"답변:\", result['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.423141Z","iopub.execute_input":"2024-11-06T03:33:31.423617Z","iopub.status.idle":"2024-11-06T03:33:31.555002Z","shell.execute_reply.started":"2024-11-06T03:33:31.423565Z","shell.execute_reply":"2024-11-06T03:33:31.553646Z"}},"outputs":[{"name":"stdout","text":"질문: 오늘 날씨 어때?\n답변: 맑고 따뜻한 기온이\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# search context in VectorStore\ndef retrieve_context(question, vectorstore):\n    docs = vectorstore.similarity_search(question, k=4)\n    if docs:\n        return \" \".join([doc.page_content for doc in docs])\n        # return docs[0].page_content  # return first relevant doc\n    else:\n        return None\n\n# Generate answer based on query and searched context similar to RAG chain\ndef answer_question_with_context(question, vectorstore):\n    context = retrieve_context(question, vectorstore)\n    if context:\n        result = qa_pipeline(question=question, context=context)\n        return result['answer'], context  # return answer and used source doc\n    else:\n        return \"관련 문맥을 찾지 못했습니다.\", None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.556691Z","iopub.execute_input":"2024-11-06T03:33:31.557138Z","iopub.status.idle":"2024-11-06T03:33:31.566617Z","shell.execute_reply.started":"2024-11-06T03:33:31.557096Z","shell.execute_reply":"2024-11-06T03:33:31.565007Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Example\nquestion = \"심사위원을 누가 맡았어?\"\n\nanswer, used_context = answer_question_with_context(question, vectorstore)\n\nprint(\"질문:\", question)\nprint(\"답변:\", answer)\nprint(\"사용된 문맥:\", used_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.56847Z","iopub.execute_input":"2024-11-06T03:33:31.568961Z","iopub.status.idle":"2024-11-06T03:33:31.902837Z","shell.execute_reply.started":"2024-11-06T03:33:31.568917Z","shell.execute_reply":"2024-11-06T03:33:31.901412Z"}},"outputs":[{"name":"stdout","text":"질문: 심사위원을 누가 맡았어?\n답변: 백종원과 안성재가\n사용된 문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Use Gemini+RAG","metadata":{}},{"cell_type":"code","source":"# It seems the best and simple and cost-free option when OpenAI api cannot be used.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.904985Z","iopub.execute_input":"2024-11-06T03:33:31.905918Z","iopub.status.idle":"2024-11-06T03:33:31.912362Z","shell.execute_reply.started":"2024-11-06T03:33:31.905848Z","shell.execute_reply":"2024-11-06T03:33:31.910324Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.91438Z","iopub.execute_input":"2024-11-06T03:33:31.915215Z","iopub.status.idle":"2024-11-06T03:33:52.360751Z","shell.execute_reply.started":"2024-11-06T03:33:31.915149Z","shell.execute_reply":"2024-11-06T03:33:52.358994Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# pip install google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.362965Z","iopub.execute_input":"2024-11-06T03:33:52.363439Z","iopub.status.idle":"2024-11-06T03:33:52.369738Z","shell.execute_reply.started":"2024-11-06T03:33:52.363381Z","shell.execute_reply":"2024-11-06T03:33:52.368235Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import Document\nfrom langchain.llms import OpenAI\nimport google.generativeai as genai\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.371281Z","iopub.execute_input":"2024-11-06T03:33:52.371741Z","iopub.status.idle":"2024-11-06T03:33:52.934495Z","shell.execute_reply.started":"2024-11-06T03:33:52.371698Z","shell.execute_reply":"2024-11-06T03:33:52.933149Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-API-KEY\"\ngenai_api_key = \"YOUR-API-KEY\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.936178Z","iopub.execute_input":"2024-11-06T03:33:52.93722Z","iopub.status.idle":"2024-11-06T03:33:52.942868Z","shell.execute_reply.started":"2024-11-06T03:33:52.937173Z","shell.execute_reply":"2024-11-06T03:33:52.941242Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"genai.configure(api_key=genai_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.945088Z","iopub.execute_input":"2024-11-06T03:33:52.946122Z","iopub.status.idle":"2024-11-06T03:33:52.957906Z","shell.execute_reply.started":"2024-11-06T03:33:52.94607Z","shell.execute_reply":"2024-11-06T03:33:52.956189Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# 1. Gemini model\ngemini_model = genai.GenerativeModel('gemini-1.5-flash')\n\n# 2. embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:43:44.015737Z","iopub.execute_input":"2024-11-06T03:43:44.016894Z","iopub.status.idle":"2024-11-06T03:43:46.808364Z","shell.execute_reply.started":"2024-11-06T03:43:44.016822Z","shell.execute_reply":"2024-11-06T03:43:46.806964Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\n# sample docs\ndocs = [\n    Document(page_content=\"한국어 챗봇은 자연어 처리 기술을 사용하여 사용자와 대화를 나눕니다.\", metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"인공지능을 활용한 챗봇은 여러 산업에서 사용되고 있습니다.\", metadata={\"source\": \"doc2\"}),\n    Document(page_content=\"한국어와 영어를 동시에 지원하는 챗봇이 점점 늘어나고 있습니다.\", metadata={\"source\": \"doc3\"}),\n    Document(page_content=\"챗봇은 고객 서비스를 개선하고 사용자 경험을 향상시키는 데 중요한 역할을 합니다.\", metadata={\"source\": \"doc4\"})\n]\n\n# to avoid collision with previous one\npersist_directory = \"./new_chroma_db\"\n\nvectorstore = Chroma.from_documents(splits, embedding=embedding_model, persist_directory=\"./chroma_db\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:57:21.626611Z","iopub.execute_input":"2024-11-06T03:57:21.627103Z","iopub.status.idle":"2024-11-06T03:57:22.330967Z","shell.execute_reply.started":"2024-11-06T03:57:21.627058Z","shell.execute_reply":"2024-11-06T03:57:22.329763Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# RAG using prompt\ndef rag_chatbot(question):\n    context_doc = vectorstore.similarity_search(question, k=1)\n    context = context_doc[0].page_content if context_doc else \"정보를 찾을 수 없습니다.\"\n\n    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer in a complete sentence:\"\n    # response = gemini_model(prompt)\n    \n    response = gemini_model.generate_content(prompt)\n    answer = response.candidates[0].content.parts[0].text\n\n    print(\"출처 문서:\", context)\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T04:08:30.733749Z","iopub.execute_input":"2024-11-06T04:08:30.734312Z","iopub.status.idle":"2024-11-06T04:08:30.743245Z","shell.execute_reply.started":"2024-11-06T04:08:30.734264Z","shell.execute_reply":"2024-11-06T04:08:30.741513Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# sample question\nquestion = \"챗봇이 어떤 기술을 사용하나요?\"\nresponse = rag_chatbot(question)\n\nprint(\"질문:\", question)\nprint(\"답변:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T04:08:34.638691Z","iopub.execute_input":"2024-11-06T04:08:34.639964Z","iopub.status.idle":"2024-11-06T04:08:35.236716Z","shell.execute_reply.started":"2024-11-06T04:08:34.639897Z","shell.execute_reply":"2024-11-06T04:08:35.235057Z"}},"outputs":[{"name":"stdout","text":"출처 문서: 인공지능을 활용한 챗봇은 여러 산업에서 사용되고 있습니다.\n질문: 챗봇이 어떤 기술을 사용하나요?\n답변: 챗봇은 자연어 처리(NLP), 기계 학습(ML), 딥 러닝(DL)과 같은 기술을 사용합니다. \n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}